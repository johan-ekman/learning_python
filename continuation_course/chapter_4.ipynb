{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuation Course – Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-scraping\" data-toc-modified-id=\"Web-scraping-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Web scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Website's-foundation\" data-toc-modified-id=\"Website's-foundation-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Website's foundation</a></span></li><li><span><a href=\"#Inspecting-websites\" data-toc-modified-id=\"Inspecting-websites-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Inspecting websites</a></span></li></ul></li><li><span><a href=\"#The-requests-module\" data-toc-modified-id=\"The-requests-module-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>The <code>requests</code> module</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-response-object\" data-toc-modified-id=\"The-response-object-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>The response object</a></span></li><li><span><a href=\"#Affirming-the-response-object\" data-toc-modified-id=\"Affirming-the-response-object-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Affirming the response object</a></span></li><li><span><a href=\"#Accessing-content-–-the-binary-string\" data-toc-modified-id=\"Accessing-content-–-the-binary-string-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Accessing content – the binary string</a></span></li><li><span><a href=\"#Downloading-a-plaintext-file\" data-toc-modified-id=\"Downloading-a-plaintext-file-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Downloading a plaintext file</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploring-the-text-data\" data-toc-modified-id=\"Exploring-the-text-data-4.2.4.1\"><span class=\"toc-item-num\">4.2.4.1&nbsp;&nbsp;</span>Exploring the text data</a></span></li></ul></li><li><span><a href=\"#Downloading-and-working-with-csv-files\" data-toc-modified-id=\"Downloading-and-working-with-csv-files-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>Downloading and working with csv files</a></span><ul class=\"toc-item\"><li><span><a href=\"#(Optional)-Exploring-and-converting-csv-data\" data-toc-modified-id=\"(Optional)-Exploring-and-converting-csv-data-4.2.5.1\"><span class=\"toc-item-num\">4.2.5.1&nbsp;&nbsp;</span>(Optional) Exploring and converting csv data</a></span></li></ul></li><li><span><a href=\"#Downloading-and-working-with-json-files\" data-toc-modified-id=\"Downloading-and-working-with-json-files-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>Downloading and working with json files</a></span></li></ul></li><li><span><a href=\"#Scraping-html-elements-using-BeautifulSoup\" data-toc-modified-id=\"Scraping-html-elements-using-BeautifulSoup-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Scraping html elements using <code>BeautifulSoup</code></a></span></li><li><span><a href=\"#Creating-scraping-loops\" data-toc-modified-id=\"Creating-scraping-loops-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Creating scraping loops</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-all-element-tags-–-.find_all()\" data-toc-modified-id=\"Find-all-element-tags-–-.find_all()-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Find all element tags – <code>.find_all()</code></a></span></li><li><span><a href=\"#Scraping-the-html-<table>\" data-toc-modified-id=\"Scraping-the-html-<table>-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Scraping the html <code>&lt;table&gt;</code></a></span></li><li><span><a href=\"#Scraping-the-county-names\" data-toc-modified-id=\"Scraping-the-county-names-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Scraping the county names</a></span></li><li><span><a href=\"#Unicode-garbage-–-weird-text-characters\" data-toc-modified-id=\"Unicode-garbage-–-weird-text-characters-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>Unicode garbage – weird text characters</a></span></li><li><span><a href=\"#Both-links-and-names-in-one-loop\" data-toc-modified-id=\"Both-links-and-names-in-one-loop-4.4.5\"><span class=\"toc-item-num\">4.4.5&nbsp;&nbsp;</span>Both links and names in one loop</a></span></li><li><span><a href=\"#Best-practice-–-put-it-in-functions\" data-toc-modified-id=\"Best-practice-–-put-it-in-functions-4.4.6\"><span class=\"toc-item-num\">4.4.6&nbsp;&nbsp;</span>Best practice – put it in functions</a></span></li><li><span><a href=\"#Finally,-scrape-it-all\" data-toc-modified-id=\"Finally,-scrape-it-all-4.4.7\"><span class=\"toc-item-num\">4.4.7&nbsp;&nbsp;</span>Finally, scrape it all</a></span></li></ul></li><li><span><a href=\"#Exercise-–-Scrape-the-Olympic-Medals\" data-toc-modified-id=\"Exercise-–-Scrape-the-Olympic-Medals-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Exercise – Scrape the Olympic Medals</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we're going to cover the basics of web scraping. We will not go through more advanced web scraping techniques, such as using Selenium. Here, we will learn how to access web pages' element trees and scrape information of it. \n",
    "\n",
    "You should know, such scraping has some prerequistes. In the introduction, I will briefly show how to research a web page's element tree, and introduce some basic concepts. There are also some helpful links to get you some more background. Just reading through these links will help you a lot. If you're completely new to these subjects, I recommend having the links open in other tabs in your web browser to be referenced as you read through this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Website's foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All web pages you frequent are the result of code. There is what's called **the frontend** of web pages. This is the visual representation of a website and is (usually) the results of three code languages. Here's a very (very) basic and rough explination:\n",
    "\n",
    "***\n",
    "**The website's building blocks – HTML**\n",
    "\n",
    "The basic building blocks of a web page is created in Hyper Text Markup Language, or HTML. It defines what \"blocks\" of content that should exist on the webpage. These blocks are called HTML-elements.\n",
    "***\n",
    "\n",
    "**The website's style and estetic – CSS**\n",
    "\n",
    "Using CSS code, websites target the html elements and changes their look and how they are ordered in relation to one another. For example, HTML code can create three text paragraphs. CSS can style them by having the same font, and organise them by ordering them horizontally instead of below one another.\n",
    "***\n",
    "\n",
    "**The website's logic – JavaScript**\n",
    "\n",
    "Modern websites's content isn't static. They often change appearance, have content move as you scroll, or have content that isn't available in some circumstances. They are dynamic. And this dynamic usually depend on some kind of logic.\n",
    "***\n",
    "\n",
    "A very basic example is when you click a link to content not appropriate for children. A textbox usually appear asking you to put in your date of birth. If you're above a certain age, the webpage will let you access its content. If not, you'll be told you're too young to access the webpage. This is a logic that (probably) is built with JavaScript. JavaScript is a programming language that developers use to apply logic to websites.\n",
    "\n",
    "_(There's also **the backend** of websites. This is the logic built to get the correct data to you as a user. Usually this is written in a programming language such as PHP)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a website's html code: this wikipedia page with [all olympic medals of all time](https://en.wikipedia.org/wiki/All-time_Olympic_Games_medal_table). We can do this by right clicking anywhere on the website and choosing \"Inspect\" (Google Chrome) or \"Inspect element\" (Firefox) in the dropdown menu that appears. [Here's a guide on how to do this on various web browsers.](https://www.lifewire.com/get-inspect-element-tool-for-browser-756549) Right click on the text paragraph up top on the article and choose \"Inspect Element\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../course_material/scraping/inspect_1.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then open **the developer tools window**. If you've never seen this window before, it may come across as the controls of a space shuttle. But don't fret! It isn't that complicated, at least the stuff we're interested in. \n",
    "\n",
    "In the developer tools window, if you're in Google Chrome, open the \"Elements\" tab, or \"Inspector\" if you're using Firefox. In the guide linked above, you can see what it's called in various browsers. The first arrow (1) points to the elements inspector tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../course_material/scraping/inspect_2.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second arrow (2) points to various html-elements that exists on the webpage. The one marked in blue is the element that was right clicked on (the visuals varies across different browsers, but the functionality is basically the same). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll through the list, you can see the entire page's structure. There are small grey arrow heads to the left of most of the elements. This indicates a tree of objects. If you click an arrow, it will unfold the tree, and you can see all elements beneath the one you clicked: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../course_material/scraping/inspect_4.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, pressing the arrow to the left of the paragraph element `<p>` (1). A tree of html-elements beneath that paragraph then unfolded (2). Unfolding the tree shows us all elements beneath the paragraph (3). These elements beneath are the **child elements**, they are **the children** of the paragraph element `<p>`. Likewise, `<p>` is **the parent element** of the elements directly beneath it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you will learn how to use Python to scrape the data that are attached to these html-elements. But first, let's learn how to download files from the internet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `requests` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So let's start with a very simple way to use code and access a web page's content. One of the most basic interactions you can do with a web page is download a file. The module `requests` is perfect for this – it makes it easy to quickly access a web page's content. Let's start with accessing a plaintext file.\n",
    "\n",
    "[On this page](http://www.textfiles.com/etext/FICTION/), we can download hundreds of classical books in plaintext format! Let's have a look at Jules Verne's book [_Around the World in 80 Days_](http://www.textfiles.com/etext/FICTION/80day10.txt). But first, we will import the `requests` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plaintext file is at this url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.textfiles.com/etext/FICTION/80day10.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to scrape the contents of the url, we need to create a response object. Why is it called \"response\"? Well, each time you access a web page through a browser, the browser calls the web page's server – it sends _a request_ – asking for content. The server then _responds_ to your browser's request, and provides content.\n",
    "\n",
    "When we use the `requests` module, we bypass the intermediary web browser by using the method `.get()`. This method sends a request directly to the server. If the url exists, it returns a response object, which we'll save into a variable. Let's name it `res`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affirming the response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a response object, we can use the `.raise_for_status()` method. This method raises an exception if the url you passed the `.get()` method doesn't work. If there is no issue with the url, `.raise_for_status()` will return `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, no issues with the url. But if we give it a url that doesn't exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://www.textfiles.com/etext/FICTION/aölasöd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://www.textfiles.com/etext/FICTION/a%C3%B6las%C3%B6d",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cd6be6b74546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/learning_python-GVvhfwh6/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://www.textfiles.com/etext/FICTION/a%C3%B6las%C3%B6d"
     ]
    }
   ],
   "source": [
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is common practice to always type this code so that you're sure that the code connected with the url successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing content – the binary string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object has an attribute called `content`. This is the entire web page's content retrieved as a binary string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Around the World in 80 Days, by Jules Verne\\r\\n\\r\\n\\r\\nAROUND THE WORLD IN EIGHTY DAYS\\r\\n\\r\\nChapter I\\r\\n\\r\\nIN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER,\\r\\nTHE ONE AS MASTER, THE OTHER AS MAN\\r\\n\\r\\n\\r\\nMr.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We havn't covered binary strings so far in the course, and I won't cover it now in great detail. Binary strings are text strings of plain binary data. You can always create your own by typing a lower cased \"b\" in front of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'This is a string'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\"This is a string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\"This is a string\" == \"This is a string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode binary strings to ordinary strings using the `.decode()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_string = b\"This is a string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a string'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_string.decode(\"utf-8\") # here we converting to unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding of strings is not part of this course. But it is _how to convert text to binary data, or vice versa._ There are hundreds of encodings that helps our computers to transform different languages characters into binary code. The first, and (perhaps) most famous being ASCII, or `\"ascii\"` as an encoding argument in Python. If you're interested in more, [read this!](https://realpython.com/python-encodings-guide/#enter-unicode) In fact, that whole article is great for understanding more about this subject.\n",
    "\n",
    "Here, we'll just go with the unicode encoding argument \"urf-8\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might all seem a bit confusing. But just know that there are something called binary strings. They are usually used when computers need to convert text data from one computer language to another. Such as when we want content from a server on the web returned to us as a string we can use in Python.\n",
    "\n",
    "Let's save the site's content to a variable and decode it into a normal string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375033"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading a plaintext file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the text file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.textfiles.com/etext/FICTION/80day10.txt\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "content = res.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around the World in 80 Days, by Jules Verne\r\n",
      "\r\n",
      "\r\n",
      "AROUND THE WORLD IN EIGHTY DAYS\r\n",
      "\r\n",
      "Chapter I\r\n",
      "\r\n",
      "IN WHICH PHILEAS FOGG AND PASSEPARTOUT ACCEPT EACH OTHER,\r\n",
      "THE ONE AS MASTER, THE OTHER AS MAN\r\n",
      "\r\n",
      "\r\n",
      "Mr. Phileas Fogg lived, in 1872, at No. 7, Saville Row, Burlington\r\n",
      "Gardens, the house in which Sheridan died in 1814.  He was one of\r\n",
      "the most noticeable members of the Reform Club, though he seemed\r\n",
      "always to avoid attracting attention; an enigmatical personage,\r\n",
      "about whom little was known, except that he was a polished man\r\n",
      "of the world.\n"
     ]
    }
   ],
   "source": [
    "print(content[:540])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the content is simply a plaintext file. No more. It actually sais so in the url we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.textfiles.com/etext/FICTION/80day10.txt'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"http://www.textfiles.com/etext/FICTION/80day10.txt\" # .txt :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there are no other content than the file's content in our response object. Let's try another page to show you what I mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<HTML>\\n<BODY BGCOLOR=\"#000000\" TEXT=\"#FFFFFF\" LINK=\"#FFFFFF\" ALINK=\"#FFFFFF\"\\n      VLINK=\"#FFFFFF\">\\n\\n<CENTER>\\n<IMG SRC=\"images/etext.jpg\"><P>\\n<P>\\n<TABLE WIDTH=90%>\\n<TR>\\n<TD WIDTH=50% VALIGN=TOP>\\n<BLOCKQUOTE>\\n<FONT SIZE=+1><B><A STYLE=\"text-decoration'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(\"http://www.textfiles.com/etext/\").content[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the requests `.get()` method retrieves all content on a site, it usually returns the web page's html tree, with all its html-elements. But since this is a plaintextfile, we dont't have to bother with that. We can straight away download the contents to a file on our hard drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"around_the_world_in_80_days.txt\", \"w\")\n",
    "file.write(content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `open()` function has the argument \"wb\" – \"write binary\" – we don't need to convert the binary string to a normal string. We can just write the file straight from the response object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"around_the_world_in_80_days.txt\", \"wb\")\n",
    "file.write(res.content)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we decoded the content into a normal Python text string, we can use regular expressions and explore the text! For example, how many times does the name \"Phileas Fogg\" occur in the book? Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"Phileas Fogg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(pattern, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about just his surname \"Fogg\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"Fogg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(pattern, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! So way more. My guess is that, since it is an old book, his name is usually written together with \"Mr\". Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"Mr\\.* Fogg\" # searching for a literal dot '.' here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(pattern, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems about right. What is the word that usually preceds \"Fogg\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(\\w+)\\W*\\s*Fogg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "preceding_word = re.findall(pattern, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `set()` function to get all unique values in a list. Let's check out all words that preceds \"Fogg\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Captain',\n",
       " 'Kong',\n",
       " 'Monsieur',\n",
       " 'Mr',\n",
       " 'Phileas',\n",
       " 'The',\n",
       " 'and',\n",
       " 'arrest',\n",
       " 'courageous',\n",
       " 'crew',\n",
       " 'face',\n",
       " 'had',\n",
       " 'her',\n",
       " 'if',\n",
       " 'intractable',\n",
       " 'joy',\n",
       " 'letting',\n",
       " 'lose',\n",
       " 'not',\n",
       " 'of',\n",
       " 'pursuing',\n",
       " 'retorted',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'that',\n",
       " 'the',\n",
       " 'to',\n",
       " 'tranquil',\n",
       " 'under'}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(preceding_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun to explore, even though it is pretty useless information :)\n",
    "\n",
    "But hopefully, you now feel a bit introduced on how to scrape text files from the internet and then exploring their content using code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this and run if you want to delete the text file:\n",
    "#from pathlib import Path\n",
    "#Path(\"around_the_world_in_80_days.txt\").unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and working with csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Downloading litterature is all fine and dandy. But what about downloading data files? Here, I will show you how to use the requests library to obtain two such types of files – the json and the csv.\n",
    "\n",
    "Let's start with **the csv file**. \"csv\" stands for \"comma separated values\". It is essentially files with rows and columns of data, where each row is separated by a newline character `\\n`, and each column by a comma `,`. Here's a url to a csv file with of covid data from Johns Hopkins University:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-29-2020.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's get it using the requests library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'FIPS,Admin2,Province_State,Country_Region,Last_Update,Lat,Long_,Confirmed,Deaths,Recovered,Active,Combined_Key,Incidence_Rate,Case-Fatality_Ratio\\n,,,Afghanistan,2020-10-30 04:24:49,33.93911,67.709953,41268,1532,34239,5497,Afghanistan,106.01016878679728,3.7123194727149365\\n,,,Albania,2020-10-30 04:24:49,41.1533,20.1683,20315,499,11007,8809,Albania,705.9211897977623,2.4563130691607187\\n,,,Algeria,2020-10-30 04:24:49,28.0339,1.6596,57332,1949,39635,15748,Algeria,130.74261426347374,3.399497662736343\\n,'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556837"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have all data in i gigantic binary string. Let's save it into a csv-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"covid_data.csv\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556837"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `csv` module to parse the data! Let's import the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv module has the method `.reader()` that takes a csv file object as an argument. The method then parses the data into rows, where each row is a list of string elements found in the data file. The result is returned as a new \"reader\" object. Let me show you what I mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"covid_data.csv\",\"r\")\n",
    "csv_reader = csv.reader(csv_file,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x1304a3120>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reader object contains all rows in the csv file. The first row is the header with the column names, the rest is the rows of data. To have all the rows returned, we can use the `list()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['FIPS',\n",
       "  'Admin2',\n",
       "  'Province_State',\n",
       "  'Country_Region',\n",
       "  'Last_Update',\n",
       "  'Lat',\n",
       "  'Long_',\n",
       "  'Confirmed',\n",
       "  'Deaths',\n",
       "  'Recovered',\n",
       "  'Active',\n",
       "  'Combined_Key',\n",
       "  'Incidence_Rate',\n",
       "  'Case-Fatality_Ratio'],\n",
       " ['',\n",
       "  '',\n",
       "  '',\n",
       "  'Afghanistan',\n",
       "  '2020-10-30 04:24:49',\n",
       "  '33.93911',\n",
       "  '67.709953',\n",
       "  '41268',\n",
       "  '1532',\n",
       "  '34239',\n",
       "  '5497',\n",
       "  'Afghanistan',\n",
       "  '106.01016878679728',\n",
       "  '3.7123194727149365']]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(csv_reader)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Optional) Exploring and converting csv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, how do we transform this data into a data structure we can explore? Well, we could convert it to a unicode encoded string value, and then use string methods to create a data structure we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-29-2020.csv\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.content.decode('utf-8') # converting data into python string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we don't have the data saved to a csv file, and want the csv module to read the data straight from our response's content, we need to split the string on all newlines. Otherwise, it will confuse what's what in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split(\"\\n\")\n",
    "csv_reader = csv.reader(data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(csv_reader) # converting the reader object to a list with all rows as items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item in the list seem to be the header. Let's save it to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FIPS',\n",
       " 'Admin2',\n",
       " 'Province_State',\n",
       " 'Country_Region',\n",
       " 'Last_Update',\n",
       " 'Lat',\n",
       " 'Long_',\n",
       " 'Confirmed',\n",
       " 'Deaths',\n",
       " 'Recovered',\n",
       " 'Active',\n",
       " 'Combined_Key',\n",
       " 'Incidence_Rate',\n",
       " 'Case-Fatality_Ratio']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header # This is all column names in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loop over all rows and match each row of data with our header to then pair with row values. We'll save these as dictionaries and then append them to a `results` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] # place holder to fill with row data\n",
    "\n",
    "# skipping first item in the list, since that's the \"header\":\n",
    "for row in data[1:]:\n",
    "    row_dict = dict(zip(header, row)) # Connect each row's data with the header as keys\n",
    "    results.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FIPS': '',\n",
       " 'Admin2': '',\n",
       " 'Province_State': '',\n",
       " 'Country_Region': 'Afghanistan',\n",
       " 'Last_Update': '2020-10-30 04:24:49',\n",
       " 'Lat': '33.93911',\n",
       " 'Long_': '67.709953',\n",
       " 'Confirmed': '41268',\n",
       " 'Deaths': '1532',\n",
       " 'Recovered': '34239',\n",
       " 'Active': '5497',\n",
       " 'Combined_Key': 'Afghanistan',\n",
       " 'Incidence_Rate': '106.01016878679728',\n",
       " 'Case-Fatality_Ratio': '3.7123194727149365'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the data into csv file, we can use the `.writer()` method. This returns a csv writer object, and to write all the data to it, we need to do so row by row. \n",
    "\n",
    "The writer object has some really handy parameters when writing csv files. For example, we can bypass the potential problem of creating a csv data file where data points include commas. So if one data point in the cell is a name, and that name is written `Ekman, Johan` – this could create future problems. If we include the parameter `quotechar='\"'` in our writer method, the method will automatically enclose data points that contain the csv delimiter character within quote marks. So `Ekman, Johan` would be saved as `\"Ekman, Johan\"` in the csv file. The parameter `quoting=csv.QUOTE_MINIMAL`, sets it so that these quotes is only added if a delimiter character is found in data points.\n",
    "\n",
    "\n",
    "But first, before writing all data, we need to create a new file. Let's do so with the context manager pattern, using the `with` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"covid_data.csv\", \"w\") as csv_file:\n",
    "    \n",
    "    # when many paramters, I usually put them on their own rows, looks nicer :)\n",
    "    covid_writer = csv.writer(csv_file,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in results:\n",
    "        covid_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! You can check the file if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to recap what we just did:\n",
    "\n",
    "1. converted the response object's content into a string\n",
    "2. splited the string on all newline `\\n` characters to get a list of all rows of data\n",
    "3. in a loop, we splited each row on all commas in the to get each row's columns\n",
    "4. created a zip object by passing the header data and the sequence's data to the `zip()` function, then passed the zip object to the `dict()` function, creating a dictonary. \n",
    "5. Saved the data in a csv file, using the `.writer()` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I havn't introduced you to the `pandas` module yet, but I will now show you how to use pandas to save the data as a excel file. First import pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(It is common practice to import pandas as a synonym `pd`, the reason being that there are so many methods used from the module while working with it, that it is more readable to use the short `pd`)_\n",
    "\n",
    "`pandas` transforms data into `series` objects. They are basically lists. If we have more than one series organised together, this is called a pandas dataframe. To create a dataframe object, we use the `DataFrame` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.DataFrame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing our data structure to the dataframe class will create a new dataframe object. Let's do so and then save it to a variable called `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datframes have the `.head()` method that shows us the first 5 rows in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incidence_Rate</th>\n",
       "      <th>Case-Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>33.93911</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>41268</td>\n",
       "      <td>1532</td>\n",
       "      <td>34239</td>\n",
       "      <td>5497</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>106.01016878679728</td>\n",
       "      <td>3.7123194727149365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Albania</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>41.1533</td>\n",
       "      <td>20.1683</td>\n",
       "      <td>20315</td>\n",
       "      <td>499</td>\n",
       "      <td>11007</td>\n",
       "      <td>8809</td>\n",
       "      <td>Albania</td>\n",
       "      <td>705.9211897977623</td>\n",
       "      <td>2.4563130691607187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Algeria</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>28.0339</td>\n",
       "      <td>1.6596</td>\n",
       "      <td>57332</td>\n",
       "      <td>1949</td>\n",
       "      <td>39635</td>\n",
       "      <td>15748</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>130.74261426347374</td>\n",
       "      <td>3.399497662736343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Andorra</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>42.5063</td>\n",
       "      <td>1.5218</td>\n",
       "      <td>4567</td>\n",
       "      <td>73</td>\n",
       "      <td>3260</td>\n",
       "      <td>1234</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>5910.8263767553235</td>\n",
       "      <td>1.5984234727392161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Angola</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>-11.2027</td>\n",
       "      <td>17.8739</td>\n",
       "      <td>10269</td>\n",
       "      <td>275</td>\n",
       "      <td>3736</td>\n",
       "      <td>6258</td>\n",
       "      <td>Angola</td>\n",
       "      <td>31.244800900424714</td>\n",
       "      <td>2.677962800662187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FIPS Admin2 Province_State Country_Region          Last_Update       Lat  \\\n",
       "0                               Afghanistan  2020-10-30 04:24:49  33.93911   \n",
       "1                                   Albania  2020-10-30 04:24:49   41.1533   \n",
       "2                                   Algeria  2020-10-30 04:24:49   28.0339   \n",
       "3                                   Andorra  2020-10-30 04:24:49   42.5063   \n",
       "4                                    Angola  2020-10-30 04:24:49  -11.2027   \n",
       "\n",
       "       Long_ Confirmed Deaths Recovered Active Combined_Key  \\\n",
       "0  67.709953     41268   1532     34239   5497  Afghanistan   \n",
       "1    20.1683     20315    499     11007   8809      Albania   \n",
       "2     1.6596     57332   1949     39635  15748      Algeria   \n",
       "3     1.5218      4567     73      3260   1234      Andorra   \n",
       "4    17.8739     10269    275      3736   6258       Angola   \n",
       "\n",
       "       Incidence_Rate Case-Fatality_Ratio  \n",
       "0  106.01016878679728  3.7123194727149365  \n",
       "1   705.9211897977623  2.4563130691607187  \n",
       "2  130.74261426347374   3.399497662736343  \n",
       "3  5910.8263767553235  1.5984234727392161  \n",
       "4  31.244800900424714   2.677962800662187  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't go through the pandas library in detail here, I just wanted to show you how it can be used to save data to excel files. Dataframe objects have the `.to_excel()` method. If we call this on our dataframe, it will save the data into an excelfile at the provided path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"covid_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a very powerful library. Using `pandas`, we don't need the `csv` module at all. We can actually just pass the url straight into the `.read_csv()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/10-29-2020.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long_</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Incidence_Rate</th>\n",
       "      <th>Case-Fatality_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>33.93911</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>41268</td>\n",
       "      <td>1532</td>\n",
       "      <td>34239</td>\n",
       "      <td>5497.0</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>106.010169</td>\n",
       "      <td>3.712319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>41.15330</td>\n",
       "      <td>20.168300</td>\n",
       "      <td>20315</td>\n",
       "      <td>499</td>\n",
       "      <td>11007</td>\n",
       "      <td>8809.0</td>\n",
       "      <td>Albania</td>\n",
       "      <td>705.921190</td>\n",
       "      <td>2.456313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>28.03390</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>57332</td>\n",
       "      <td>1949</td>\n",
       "      <td>39635</td>\n",
       "      <td>15748.0</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>130.742614</td>\n",
       "      <td>3.399498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>42.50630</td>\n",
       "      <td>1.521800</td>\n",
       "      <td>4567</td>\n",
       "      <td>73</td>\n",
       "      <td>3260</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>5910.826377</td>\n",
       "      <td>1.598423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>2020-10-30 04:24:49</td>\n",
       "      <td>-11.20270</td>\n",
       "      <td>17.873900</td>\n",
       "      <td>10269</td>\n",
       "      <td>275</td>\n",
       "      <td>3736</td>\n",
       "      <td>6258.0</td>\n",
       "      <td>Angola</td>\n",
       "      <td>31.244801</td>\n",
       "      <td>2.677963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS Admin2 Province_State Country_Region          Last_Update       Lat  \\\n",
       "0   NaN    NaN            NaN    Afghanistan  2020-10-30 04:24:49  33.93911   \n",
       "1   NaN    NaN            NaN        Albania  2020-10-30 04:24:49  41.15330   \n",
       "2   NaN    NaN            NaN        Algeria  2020-10-30 04:24:49  28.03390   \n",
       "3   NaN    NaN            NaN        Andorra  2020-10-30 04:24:49  42.50630   \n",
       "4   NaN    NaN            NaN         Angola  2020-10-30 04:24:49 -11.20270   \n",
       "\n",
       "       Long_  Confirmed  Deaths  Recovered   Active Combined_Key  \\\n",
       "0  67.709953      41268    1532      34239   5497.0  Afghanistan   \n",
       "1  20.168300      20315     499      11007   8809.0      Albania   \n",
       "2   1.659600      57332    1949      39635  15748.0      Algeria   \n",
       "3   1.521800       4567      73       3260   1234.0      Andorra   \n",
       "4  17.873900      10269     275       3736   6258.0       Angola   \n",
       "\n",
       "   Incidence_Rate  Case-Fatality_Ratio  \n",
       "0      106.010169             3.712319  \n",
       "1      705.921190             2.456313  \n",
       "2      130.742614             3.399498  \n",
       "3     5910.826377             1.598423  \n",
       "4       31.244801             2.677963  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, save it as a csv file on our hard drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('covid_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and working with json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another data file that you will encounter many times on the internet is the JSON file. It stands for JavaScript Object Notation, since it is how data is stored in the JavaScript language. Json files are in their essence (just like csv files) just plaintext files, but ordered in a specific way. JavaScript and Python's data structures are pretty similar, and that's why when you see a json file, it looks like a dictionary – or a list of dictionaries. \n",
    "\n",
    "Here's an example of a json file to show you how much they look like Python's data structures. Here's Sweden's population 2016-2019 in a json string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = \"\"\"\n",
    "[{\"country\": \"Sweden\",\"year\": 2016,\"value\": 9995153.0},\n",
    "{\"country\": \"Sweden\",\"year\": 2017,\"value\": 10120242.0},\n",
    "{\"country\": \"Sweden\",\"year\": 2018,\"value\": 10230185.0},\n",
    "{\"country\": \"Sweden\",\"year\": 2019,\"value\": 10327589.0}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{\"country\": \"Sweden\",\"year\": 2016,\"value\": 9995153.0},\n",
      "{\"country\": \"Sweden\",\"year\": 2017,\"value\": 10120242.0},\n",
      "{\"country\": \"Sweden\",\"year\": 2018,\"value\": 10230185.0},\n",
      "{\"country\": \"Sweden\",\"year\": 2019,\"value\": 10327589.0}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json files data structures are identical to Python's, only that they are string values. So we need to convert them from json data into Python data structures. This is easiest with the `json` module – a module within the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `.loads()` lets us convert a json string into a Python data object. Just pass the string to the method as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'country': 'Sweden', 'year': 2016, 'value': 9995153.0},\n",
       " {'country': 'Sweden', 'year': 2017, 'value': 10120242.0},\n",
       " {'country': 'Sweden', 'year': 2018, 'value': 10230185.0},\n",
       " {'country': 'Sweden', 'year': 2019, 'value': 10327589.0}]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a json file from the internet! [Here is a json file](https://opendata.ecdc.europa.eu/covid19/casedistribution/json/) with covid data, [provided by the ECDC](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) (European Centre for Desease Prevention and Control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://opendata.ecdc.europa.eu/covid19/casedistribution/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object's content is a gigantic binary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\\r\\n   \"records\" : [\\r\\n      {\\r\\n         \"dateRep\" : \"02/11/2020\",\\r\\n         \"day\" : \"02\",\\r\\n         \"month\" : \"11\",\\r\\n         \"year\" : \"2020\",\\r\\n         \"cases\" : 132,\\r\\n         \"deaths\" : 5,\\r\\n         \"countriesAndTerritories\" : \"Afghanistan\",\\r\\n         \"geoId\" : \"AF\",\\r\\n         \"countryterritoryCode\" : \"AFG\",\\r\\n         \"popData2019\" : 38041757,\\r\\n         \"continentExp\" : \"Asia\",\\r\\n         \"Cumulative_number_for_14_days_of_COVID-19_cases_per_100000\" : \"3.76691329\"\\r\\n      },\\r\\n      {\\r\\n         \"d'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the data to a Python data structure using the `json.dumps()` method. But first, we need to convert the content to a regular Python string value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json was successfully converted into a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is stored as the value of the key \"records\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['records'])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dateRep': '02/11/2020',\n",
       "  'day': '02',\n",
       "  'month': '11',\n",
       "  'year': '2020',\n",
       "  'cases': 132,\n",
       "  'deaths': 5,\n",
       "  'countriesAndTerritories': 'Afghanistan',\n",
       "  'geoId': 'AF',\n",
       "  'countryterritoryCode': 'AFG',\n",
       "  'popData2019': 38041757,\n",
       "  'continentExp': 'Asia',\n",
       "  'Cumulative_number_for_14_days_of_COVID-19_cases_per_100000': '3.76691329'},\n",
       " {'dateRep': '01/11/2020',\n",
       "  'day': '01',\n",
       "  'month': '11',\n",
       "  'year': '2020',\n",
       "  'cases': 76,\n",
       "  'deaths': 0,\n",
       "  'countriesAndTerritories': 'Afghanistan',\n",
       "  'geoId': 'AF',\n",
       "  'countryterritoryCode': 'AFG',\n",
       "  'popData2019': 38041757,\n",
       "  'continentExp': 'Asia',\n",
       "  'Cumulative_number_for_14_days_of_COVID-19_cases_per_100000': '3.57501889'}]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"records\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping html elements using `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now you've practised downloading files using the `requests` library. As mentioned in previous section, requests is a brilliant way to quickly get access to a web page's content. However, it is not very practical if the content isn't a ordered data file. \n",
    "\n",
    "To show you what I mean, let's have a look at [a web page](https://www.socialstyrelsen.se/om-socialstyrelsen/pressrum/) with the Socialstyrelsen's (Sweden's National Board of Health and Welfare) press contacts. It is the agency responsible for questions of health and medical prodcedures. Here's the web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.socialstyrelsen.se/om-socialstyrelsen/pressrum/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the requests library and get a response object of the web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, have a look at the web page's content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\r\\n\\r\\n\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"sv\">\\r\\n<head>\\r\\n\\r\\n        <!-- Google Tag Manager -->\\r\\n        <script>\\r\\n            var dataLayer = dataLayer || [];\\r\\n            (function (w, d, s, l, i) {\\r\\n                w[l] = w[l] || [];\\r\\n                w[l].push({\\r\\n                    \\'gtm.start\\':\\r\\n                        new Date().getTime(),\\r\\n                    event: \\'gtm.js\\'\\r\\n                });\\r\\n                var f = d.getElementsByTagName(s)[0],\\r\\n\\r\\n                    j = d.createElement(s),\\r\\n '"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a messy soup of html code mashed into a binary string. This is not particularly ordered data to scrape web content from.\n",
    "\n",
    "Let's say we want all phone numbers scraped from this web page. We _could_ just use the requests response object. Converting it to a regular string, import the `re` module, create a regular expression, and apply this regex on the entire web page's content. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page = res.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pattern = \"(075-[\\d|\\s]+)\" # do you understand the regex? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['075-247 30 05', '075-247 30 05', '075-247 30 00', '075-247 30 00']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(num_pattern,web_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't mean we _should_ scrape data this way. One, perhaps obvious reason is that this fetches _all_ patterns that match our regex on the web page. If you look at bottom of the web page, you'll see that Socialstyrelsen has its phone number to the reception to the right. Our regex matched on the as well. \n",
    "\n",
    "My point is that this isn't a precise way to scrape information from a web page. If only there was a way to get all that messy soup of html code into a python object that we could work with..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is! It's called beautifulsoup! Or, rather, it's a module called `bs4` – bs standing four \"Beautiful soup\", 4 for version 4. The module is not part of the standard library, but it _is_ included in the Anaconda package. So, if you've installed Anaconda as recommended, you should just be able to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(If you're not using the Ananconda package, download and install the `bs4` module such that you can use it in this notebook)_\n",
    "\n",
    "The response object that was returned using requests has the entire web page's html code. The `BeautifulSoup` class within the `bs4` module can help us parse the html-string into a special Python \"soup\" object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `soup` variable now has methods that can help us sort through all the messy html code of websites! We can, as an example, search for elements with a specific class.\n",
    "\n",
    "But to be able to do so, we must know what to look for. This is when we **inspect using the developer tools** we learned about in section 4.1.2. In our example, we want to find the press contact info. Looking at the site, we see that this information is in a box with the headline \"Kontakt\". Let's right click the box and see which element this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../course_material/scraping/html_1.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you right clicked, you may have clicked on one of the children of the one I have selected in the pic above. But as you can see, the box with contact info is a `<div>` element with the class attribute \"contact\". We can search for this class in our soup object, using the `.find()` method. It searches for element types, but have the parameter `class_` where we can specify the element type's class attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "div = soup.find(\"div\", class_=\"contact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"contact\">\n",
       "<div class=\"row\"><div class=\"block contactpersonblock col-lg-12 col-md-12 col-sm-12 col-xs-12 displaymode-full\">\n",
       "<div class=\"contact-person\">\n",
       "<h2>Kontakt</h2>\n",
       "<div class=\"contact-person__name\">\n",
       "\n",
       "            Presstjänsten\n",
       "        </div>\n",
       "<div class=\"contact-person__email\">\n",
       "<span>E-post:  </span>\n",
       "<a class=\"no-border-link\" href=\"mailto:media@socialstyrelsen.se\">media@socialstyrelsen.se</a>\n",
       "</div>\n",
       "<div class=\"contact-person__phone\">\n",
       "<span>Telefonnummer:  </span>\n",
       "<a class=\"no-border-link\" href=\"tel:0752473005\">075-247 30 05</a>\n",
       "</div>\n",
       "</div></div></div>\n",
       "</div>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This html tree tells us that there are child elements of this div containing the information we want. The class \"contact-person__phone\" is what we're after. Let's get that one instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.find(\"div\", class_=\"contact-person__phone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the text of our element by calling the `.text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTelefonnummer:  \\n075-247 30 05\\n'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! We scraped the phone number we wanted! Now, let's try something a little more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating scraping loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, in this section, let's imagine we want to gather up all websites' urls of California's health care centres. We work at a Californian newspaper, and our editor wants us to check all health centres websites. There's been this new policy that require them to display health care costs directly on their wesites homepages. The newspaper wants to do a story if this policy is being followed. \n",
    "\n",
    "We can't be bothered to copy and paste all websites urls from each link by hand, so we decide to scrape the information instead! The centres contact info can be found on the California Departement of Health Care Services' website. [Here](https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices.aspx), to be exact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices.aspx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the health care offices are located at county level, and their contact info – together with a link to their website – can be found in lists. But they are located at three different web pages at the site. These pages can be found on the link above. So, how to proceed?\n",
    "\n",
    "First, let's scrape the urls to the three web pages that contain all county information. Let's save them in a list. The hyperlink elment, the `<a>` tag, is what we're looking for. Hyperlinks have a attribute `href` which specifies the url of the page the link goes to. Inspecting the site using developer tools, we see that the links are within this element:\n",
    "\n",
    "<img src=\"../course_material/scraping/html_2.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, we want to scrape the links beneath the `<ul>` tag (\"ul\" is an \"unordered list\" element). The `<ul>` has the class \"dfwp-list\". Let's get a response object, and then filter the html code using `BeautifulSoup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = soup.find(\"ul\", class_=\"dfwp-list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all element tags – `.find_all()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soup objects has a method called `.find_all()`. It searches the html tree for all element tags we provide. Let's use it to get all `<a>`-tagged elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/services/medi-cal/Pages/CountyOffices3.aspx\" target=\"\" title=\"\">County Listings: A - L</a>,\n",
       " <a href=\"/services/medi-cal/Pages/CountyOffices4.aspx\" target=\"\" title=\"\">County Listings: M - R</a>,\n",
       " <a href=\"/services/medi-cal/Pages/CountyOffices5.aspx\" target=\"\" title=\"\">County Listings: S - Z</a>]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! As you can see, the `<a>` tags have `href` attributes with the url links we need! Lets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = element.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All soup objects have the method `.get()` which lets us extract attribute information from an element. We pass the attribute we want as an argument. We need to loop over the links and extract the href-attributes. But while we're at it we can fix another problem:\n",
    "\n",
    "As you can see, the links are not complete urls. They start with `/services/...`, and the url to the website is `https://www.dhcs.ca.gov...`. Therefore, while looping over the links, let's also add this core url to the links' start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/services/medi-cal/Pages/CountyOffices3.aspx'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0].get(\"href\") # this is how we extract a specific attribute from elements, returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the beginning of all links:\n",
    "main_url = \"https://www.dhcs.ca.gov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,link in enumerate(links):\n",
    "    # First, extract href text:\n",
    "    href = link.get(\"href\")\n",
    "    # change the list items into string values with the complete urls\n",
    "    links[count] = main_url + href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices3.aspx',\n",
       " 'https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices4.aspx',\n",
       " 'https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices5.aspx']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see that all links work. We can do this looping over the links, creating response objects of them, and then using the `requests` method `.raise_for_status()`. If nothing happens, we've been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a few seconds:\n",
    "for link in links:\n",
    "    res = requests.get(link)\n",
    "    res.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! No exceptions were raised. The urls seem to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping the html `<table>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to have a look at the links themselves. All the lists with the contact info of the health care centres is within a html table `<table>`. And all these html-tables in all three url-links all have the class \"ms-rteTable-default\". Here's a pic:\n",
    "\n",
    "<img src=\"../course_material/scraping/html_3.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Let's try to find it in the first link:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(links[0])\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.content)\n",
    "\n",
    "table = soup.find(\"table\", class_=\"ms-rteTable-default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find all hyperlinks in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_links = table.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(county_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alamedasocialservices.org/public/index.cfm\n",
      "http://alpinecountyca.gov/Index.aspx?NID=191\n",
      "http://www.co.amador.ca.us/departments/health-human-services\n",
      "http://www.buttecounty.net/dess/ConnectDESS.aspx\n",
      "http://hhsa.calaverasgov.us/Public-Assistance/Eligibility/Medi-Cal\n"
     ]
    }
   ],
   "source": [
    "# printing top 5 links:\n",
    "for hyperlink in county_links[:5]:\n",
    "    print(hyperlink.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []\n",
    "\n",
    "for link in county_links:\n",
    "    all_links.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping the county names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! It worked! But, to make the data a bit more workable, let's save the link together with the county name. This is also found in the table element, but it is within a table div `<td>` and has the class \"ms-rteTableEvenCol-default\". Let's get a list of all of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_names = soup.find_all(\"td\", class_=\"ms-rteTableEvenCol-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(county_names[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"ms-rteTableEvenCol-default\" colspan=\"1\" rowspan=\"1\" style=\"width:50%;\">​<strong>County Name</strong></td>,\n",
       " <td class=\"ms-rteTableEvenCol-default\" rowspan=\"1\" style=\"width:50%;\">​Alameda County</td>,\n",
       " <td class=\"ms-rteTableEvenCol-default\" rowspan=\"1\" style=\"width:50%;\">​Alpine County</td>]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Does it work with the other tables on the other links?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(links[1])\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.content)\n",
    "\n",
    "county_names = soup.find_all(\"td\", class_=\"ms-rteTableEvenCol-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(county_names[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it doesn't. That's because the class's name is \"ms-rteTableFirstCol-default\" in the second and third tables. We need to include a if statement that will change the class name if the `soup.find_all()` method doesn't return anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_names = soup.find_all(\"td\", class_=\"ms-rteTableEvenCol-default\")\n",
    "if not county_names:\n",
    "    county_names = soup.find_all(\"td\", class_=\"ms-rteTableFirstCol-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(county_names[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Let's return to the first table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(links[0])\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.content)\n",
    "\n",
    "county_names = soup.find_all(\"td\", class_=\"ms-rteTableEvenCol-default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `<td>` is a table header, so we can skip that one. The other ones seem correct! We can get the text by using the `.text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u200bAlameda County'"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_names[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode garbage – weird text characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All text in the table starts with a special unicode character – `'\\u200b'`. This is a [\"zero length whitespace character\"](https://www.fileformat.info/info/unicode/char/200b/index.htm). Let's use the string method `.replace()` to remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alameda County'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_names[1].text.replace(\"\\u200b\", \"\") # replace \"\\u200b\" with \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may also be another unicode character – the `\"\\xa0\"` character (non-breaking space) – in some strings. We'll have to remove that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alameda County'"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can chain our string methods like this to do it all in one line:\n",
    "county_names[1].text.replace(\"\\u200b\", \"\").replace('\\xa0', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want all county names in a list to be able to use together with the links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "# skipping first td element, since that's the header:\n",
    "for td_element in county_names[1:]:\n",
    "    name = td_element.text.replace(\"\\u200b\", \"\").replace('\\xa0', '')\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alameda County', 'Alpine County', 'Amador County']"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both links and names in one loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We can now loop over the lists and create a dictionary with both county name and url! Since the length of the two lists are the same, we can do this in one go if we use the `enumerate()` function in our loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for count,name in enumerate(names):\n",
    "    county_data = {}\n",
    "    county_data['county'] = name\n",
    "    county_data['url'] = all_links[count]\n",
    "    results.append(county_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'county': 'Alameda County',\n",
       "  'url': 'https://alamedasocialservices.org/public/index.cfm'},\n",
       " {'county': 'Alpine County',\n",
       "  'url': 'http://alpinecountyca.gov/Index.aspx?NID=191'},\n",
       " {'county': 'Amador County',\n",
       "  'url': 'http://www.co.amador.ca.us/departments/health-human-services'}]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:3] # top three items in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practice – put it in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems to have worked as expected! Now, finally, we need to do this on all three links. To make this smoother, let's put all the code above into functions that we can use in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_links(soup):\n",
    "    # find the table:\n",
    "    table = soup.find(\"table\", class_=\"ms-rteTable-default\")\n",
    "    \n",
    "    # find all hyperlinks within table:\n",
    "    county_links = table.find_all(\"a\")\n",
    "    \n",
    "    # loop over all hyperlinks and saving the href attribute:\n",
    "    all_links = []\n",
    "    for link in county_links:\n",
    "        # this if-statement is because some counties may not have a hyperlink\n",
    "        if link.get(\"href\"):\n",
    "            all_links.append(link.get(\"href\"))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # return result\n",
    "    return all_links\n",
    "\n",
    "def get_county_names(soup):\n",
    "    # find all the <td> elements with the class name:\n",
    "    county_names = soup.find_all(\"td\", class_=\"ms-rteTableEvenCol-default\")\n",
    "    if not county_names:\n",
    "        county_names = soup.find_all(\"td\", class_=\"ms-rteTableFirstCol-default\")\n",
    "\n",
    "    # loop over all <td> elements, saving their text\n",
    "    names = []\n",
    "    # skipping first td element, since that's the header:\n",
    "    for td_element in county_names[1:]:\n",
    "        # also, removing weird unicode characted \"\\u200b\":\n",
    "        name = td_element.text.replace(\"\\u200b\", \"\").replace('\\xa0', '')\n",
    "        names.append(name)\n",
    "    \n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(links[2])\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.dha.saccounty.net/',\n",
       " 'http://hhsa.cosb.us/divisions/public-assistance/medi-cal/',\n",
       " 'http://hs.sbcounty.gov/tad/Pages/Apply-for-Medi-Cal.aspx']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_county_links(soup)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['San Benito County', 'San Bernardino County', 'San Diego County']"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_county_names(soup)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! It worked! Now, just need to put the loop in a function as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_and_links(soup):\n",
    "    county_names = get_county_names(soup)\n",
    "    county_links = get_county_links(soup)\n",
    "    \n",
    "    results = []\n",
    "    for count,name in enumerate(county_names):\n",
    "        data = {}\n",
    "        data['county'] = name\n",
    "        data['url'] = county_links[count]\n",
    "        results.append(data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'county': 'San Benito County', 'url': 'http://www.dha.saccounty.net/'},\n",
       " {'county': 'San Bernardino County',\n",
       "  'url': 'http://hhsa.cosb.us/divisions/public-assistance/medi-cal/'},\n",
       " {'county': 'San Diego County',\n",
       "  'url': 'http://hs.sbcounty.gov/tad/Pages/Apply-for-Medi-Cal.aspx'},\n",
       " {'county': 'City & County of San Francisco',\n",
       "  'url': 'https://www.sandiegocounty.gov/hhsa/programs/ssp/medi-cal_program/index.html'},\n",
       " {'county': 'San Joaquin County',\n",
       "  'url': 'https://www.sfhsa.org/services/health-food/medi-cal'}]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_names_and_links(soup)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the three url's with the links to the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices3.aspx',\n",
       " 'https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices4.aspx',\n",
       " 'https://www.dhcs.ca.gov/services/medi-cal/Pages/CountyOffices5.aspx']"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, scrape it all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop over these three links, create a soup object and use our `get_names_and_links()` function on it. Let's do so and save the resulting data into a complete data structure of all health care offices names and website links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_county_data = []\n",
    "for link in links:\n",
    "    res = requests.get(link)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(res.content)\n",
    "    \n",
    "    county_data = get_names_and_links(soup)\n",
    "    \n",
    "    all_county_data += county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_county_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'county': 'Alameda County',\n",
       "  'url': 'https://alamedasocialservices.org/public/index.cfm'},\n",
       " {'county': 'Alpine County',\n",
       "  'url': 'http://alpinecountyca.gov/Index.aspx?NID=191'},\n",
       " {'county': 'Amador County',\n",
       "  'url': 'http://www.co.amador.ca.us/departments/health-human-services'}]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_county_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success! We scraped all county names, and all the counties' website urls. Yay!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise – Scrape the Olympic Medals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will return to where this chapter started – the Wikipedia page of [all olympic medals of all time.](https://en.wikipedia.org/wiki/All-time_Olympic_Games_medal_table). **I want you to scrape the entire table** and save it as an csv file on your computer. So the result should be a data structure that looks like this:\n",
    "\n",
    "```\n",
    "[{\"country\":\"Afghanistan\",\"num_summer_games\":14,\"num_summer_gold\":0,\n",
    "\"num_summer_silver\":0,\"num_summer_bronze\":2,...etc},\n",
    " {\"country\":\"Algeria\",\"num_summer_games\":14,\"num_summer_gold\":5, ...}]\n",
    "```\n",
    "A list with a dictionary for each row! Save this into a csv file on your hard drive.\n",
    " \n",
    "You should first start by trying to do this using `requests` and `BeautifulSoup`. Then (if you want, this is optional), I want you to try to google how you can do it with the `pandas` library. Googling your way forward is a very important part of learning programming, and I want to see if this can egg your interest! \n",
    "\n",
    "I'll show you how I would do it with both techniques in the solutions notebook.\n",
    " \n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/All-time_Olympic_Games_medal_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
